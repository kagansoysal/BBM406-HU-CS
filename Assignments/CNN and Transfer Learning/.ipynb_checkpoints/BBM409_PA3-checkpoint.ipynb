{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEPifRWi8RB0"
   },
   "source": [
    "## BBM 409 - Programming Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRABL12L8RB4"
   },
   "source": [
    "###  GAZİ KAĞAN SOYSAL 2210356050"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qb4QIkS68RB4"
   },
   "source": [
    "## 1. Implementing a CNN from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sq8IboH58RB4"
   },
   "source": [
    "### 1.1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BenkxrRTeO7Z"
   },
   "source": [
    "**TASK:** In this task, we determine which animal the images are by applying filters that extract corner, edge, etc. features from the photos of animals using CNN architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGxuFl4seO7Z"
   },
   "source": [
    "##### What are the main components of a CNN architecture?\n",
    "- Convolutional Layers: Extract features using filters/kernels.\n",
    "- Pooling Layers: Downsample feature maps to reduce dimensionality.\n",
    "- Activation Functions: Introduce non-linearity (e.g., ReLU).\n",
    "- Fully Connected Layers: Map extracted features to output classes.\n",
    "- Dropout/Regularization: Prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAVV90fveO7Z"
   },
   "source": [
    "##### Why do we use CNNs in image classification?\n",
    "- CNNs automatically learn spatial hierarchies of features (edges, shapes, textures).\n",
    "- They reduce parameters compared to fully connected networks, improving efficiency.\n",
    "- Translation invariance via pooling layers ensures robustness to position shifts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edEYWbaUeO7Z"
   },
   "source": [
    "**Dataset:** The dataset contains a total of 4500 images, 450 for each of 10 different animal species."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-LA2-Ww8RB6"
   },
   "source": [
    "### 1.2. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CzXEXAD68RB6"
   },
   "outputs": [],
   "source": [
    "## Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import Subset, DataLoader, Dataset\n",
    "from torch.utils.data import Subset\n",
    "from collections import defaultdict\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mDRlIFmu8RB7"
   },
   "outputs": [],
   "source": [
    "## Load the dataset using PyTorch's data loading utilities\n",
    "## Apply necessary preprocessing such as resizing and normalization\n",
    "dataset = datasets.ImageFolder(root=\"/content/pa3_subset_animal\")\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.8, 1.2)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "test_val_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7m7uvVWeO7a"
   },
   "source": [
    "- Since our dataset is a small dataset, it is necessary to diversify the dataset so that the model can learn more details.\n",
    "\n",
    "- Increasing the data variety allows the model to capture finer details and learn better. That's why we apply various transformations to the train data (rotate, shift, etc.)\n",
    "\n",
    "- We also set our images to a standard size (256,256) and normalize the pixel values ​​to make more consistent calculations.\n",
    "\n",
    "- However, we do not apply these transformations for test and validation sets because we need to test the data on real-life data when testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jUmE4aoNOo8l"
   },
   "outputs": [],
   "source": [
    "class TransformedSubset(Dataset):\n",
    "    def __init__(self, original_dataset, indices, transform=None):\n",
    "        self.original_dataset = original_dataset\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.original_dataset[self.indices[idx]]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZ7ocmzoeO7b"
   },
   "source": [
    "- We use this class to apply transforms individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TrsOVltrh6GY"
   },
   "outputs": [],
   "source": [
    "class_indices = defaultdict(list)\n",
    "for idx, (image, label) in enumerate(dataset):\n",
    "    class_indices[label].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KX3N_l9ch6GY"
   },
   "outputs": [],
   "source": [
    "train_indices, val_indices, test_indices = [], [], []\n",
    "\n",
    "for label, indices in class_indices.items():\n",
    "    train_indices.extend(indices[:300])\n",
    "    val_indices.extend(indices[300:375])\n",
    "    test_indices.extend(indices[375:])\n",
    "\n",
    "train_dataset = TransformedSubset(dataset, train_indices, train_transform)\n",
    "val_dataset = TransformedSubset(dataset, val_indices, test_val_transform)\n",
    "test_dataset = TransformedSubset(dataset, test_indices, test_val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rkqlf6WaeO7b"
   },
   "source": [
    "- We take equal number of images from each class for train, validation and test set. 300 images for train set, 75 images for validation and test set from each class.\n",
    "\n",
    "- We use loaders to get data by dividing it. These loaders provide space and ease of calculation by bringing data together in batches. There are 64 images in each batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6yQkfFD8RB8"
   },
   "source": [
    "### 1.3. Define your CNN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEtX_-YZeO7b"
   },
   "source": [
    "##### Reason of Architecture:\n",
    "- 5 layers is a suitable number to provide balance between overfitting and underfitting.\n",
    "\n",
    "- Increasing number of filters allows to detect more detailed features as layers get deeper.\n",
    "\n",
    "- Applying padding as 1 and setting kernel size as 3*3 prevents data loss and pays attention to details by calculating smaller regions.\n",
    "\n",
    "- Applying pooling as 2*2 and setting stride as 1 summarizes the data and prevents data loss.\n",
    "\n",
    "- The 1st fully connected layer brings together the information obtained. The 2nd fully connected layer provides probability information about which class the image belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtzpnnbYeO7b"
   },
   "source": [
    "##### Why is used ReLU for activation function?\n",
    "- The ReLu function equates negative values ​​to 0, which creates non-linearity. Thus, more complex relationships are learned.\n",
    "- Gradients are calculated faster because the derivative can be simply taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jYEoFWPJoZ-Y"
   },
   "outputs": [],
   "source": [
    "## Design CNN architecture\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(256 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.pool(F.relu(self.conv1(X)))\n",
    "        X = self.pool(F.relu(self.conv2(X)))\n",
    "        X = self.pool(F.relu(self.conv3(X)))\n",
    "        X = self.pool(F.relu(self.conv4(X)))\n",
    "        X = self.pool(F.relu(self.conv5(X)))\n",
    "\n",
    "        X = X.view(-1, 256 *8 * 8)\n",
    "\n",
    "        X = F.relu(self.fc1(X))\n",
    "\n",
    "        X = self.fc2(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23njcboArttc"
   },
   "source": [
    "### 1.4 Prepare the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bbag5-5Eh6Ga"
   },
   "outputs": [],
   "source": [
    "cnn_model = CNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Z7dmNGprxDo"
   },
   "outputs": [],
   "source": [
    "## Define appropriate loss function for multi-class classification\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJJu_K7SeO7c"
   },
   "source": [
    "- The Cross Entropy function depends on the probability of the true class of the samples. It tries to keep the probability of the true class as high as possible. This is suitable for our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDG_al2zr76F"
   },
   "outputs": [],
   "source": [
    "## Choose an optimizer (SGD or Adam) and set its parameters (e.g., learning rate)\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.0005, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCCv52CfeO7c"
   },
   "source": [
    "- Adam Optimization Algorithm can automatically adjust the learning rate for each parameter by looking at the history of the parameter's gradients.\n",
    "\n",
    "- We define a weight decay value to prevent the weights from increasing too much. This penalizes high weights and ensures that the loss is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nMJPb_qUeO7c"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "cnn_model = cnn_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MeS2E2h9eO7c"
   },
   "source": [
    "- To train the model faster, we move the model to GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aW87Td98RB9"
   },
   "source": [
    "### 1.5 Train and Validate the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q3DK67Y8oyKr",
    "outputId": "4807b59b-8b2d-45c0-d5ea-56a7cb3c3325"
   },
   "outputs": [],
   "source": [
    "## Iterate over the training dataset in mini-batches\n",
    "## Implement forward pass, compute loss, and backward pass for gradient computation\n",
    "## Update model parameters using the optimizer based on computed gradients\n",
    "## Validate the model on the validation set periodically and plot the validation loss\n",
    "## Repeat the training process for a suitable number of epochs (at least 30epochs)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "true_prediction = 0\n",
    "total = 0\n",
    "\n",
    "epochs = 40\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    cnn_model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = cnn_model(inputs)\n",
    "\n",
    "        loss = loss_func(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    val_running_loss = 0.0\n",
    "    cnn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "            val_outputs = cnn_model(val_inputs)\n",
    "            val_loss = loss_func(val_outputs, val_labels)\n",
    "            val_running_loss += val_loss.item()\n",
    "\n",
    "            _, predicted_target = torch.max(val_outputs.data, 1)\n",
    "            total += val_labels.size(0)\n",
    "            true_prediction += (predicted_target == val_labels).sum().item()\n",
    "\n",
    "    val_loss = val_running_loss / len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    val_accuracy = 100 * true_prediction / total\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "XFARBj76n75X",
    "outputId": "ab25d7e7-896d-43fa-9a42-9709653ea44b"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMpyY4ZHeO7d"
   },
   "source": [
    "- The loss values ​​in the train set and validation set are as follows. The train loss value decreases as the model adapts to the train set. The validation loss value remains constant after a point. This indicates that the model cannot learn any more and if it starts to increase, overfitting occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86YzyL-xr-Z6"
   },
   "source": [
    "- The learning rate we chose (0.0005) is an optimal value for model training speed and balanced progress.\n",
    "\n",
    "- We chose a batch size of 64 because a smaller batch size can cause the gradients to be noisier and the model to perform slower. A larger batch size increases the computational cost in RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "aiqcJh6XgIxF",
    "outputId": "4ee973de-1b6e-482e-d1f0-e2f058119005"
   },
   "outputs": [],
   "source": [
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVgbSxQaeO7d"
   },
   "source": [
    "- Validation accuracy increases as the training process progresses, and after a certain point, the rate of increase slows down. At this point, we finish the training process while our model is at its best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Snzyw25wy2qD"
   },
   "source": [
    "### 1.6 Evaluate the trained model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QzZCISJRy7eR",
    "outputId": "b62667a2-e282-4c71-92cb-018f874b16f7"
   },
   "outputs": [],
   "source": [
    "## Test the trained model on the test set to evaluate its performance\n",
    "\n",
    "cnn_model.eval()\n",
    "\n",
    "true_prediction = 0\n",
    "total = 0\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = cnn_model(inputs)\n",
    "        _, predicted_target = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        true_prediction += (predicted_target == labels).sum().item()\n",
    "\n",
    "        all_preds.extend(predicted_target.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(f\"Accuracy: {100 * true_prediction / total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "id": "BVpph5-VHgSV",
    "outputId": "38c12fb8-e27d-460a-fa17-afcff0784944"
   },
   "outputs": [],
   "source": [
    "## Compute metrics such as accuracy, precision, recall, and F1-score to assess classification performance.\n",
    "report_cnn = classification_report(all_labels, all_preds, target_names=['cane', 'cavallo', 'elefante', 'farfalla', 'gallina', 'gatto', 'mucca', 'pecora', 'ragno', 'scoiattolo'], output_dict = True)\n",
    "df_report_cnn = pd.DataFrame(report_cnn).transpose()\n",
    "df_report_cnn.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "leWIal6GeO7e",
    "outputId": "dd0aed19-ec84-408c-9202-226a0b660a27"
   },
   "outputs": [],
   "source": [
    "## Visualize confusion matrix to understand the model's behavior across different classes\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['cane', 'cavallo', 'elefante', 'farfalla', 'gallina', 'gatto', 'mucca', 'pecora', 'ragno', 'scoiattolo'], yticklabels=['cane', 'cavallo', 'elefante', 'farfalla', 'gallina', 'gatto', 'mucca', 'pecora', 'ragno', 'scoiattolo'])\n",
    "plt.ylabel('Real Labels')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCPD35O9zPri"
   },
   "source": [
    "### 1.7 Conclusion and interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45mWFbj2bYLr"
   },
   "source": [
    "- Our evaluation on the test set was 59.3%. This is an average value. It shows that the model learned the general features but could not learn the fine details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YLydb6rbu_z"
   },
   "source": [
    "- In the training phase, the small size of the data made it difficult to train the model. We applied various transformations to diversify the data. While applying these transformations, it was important to determine which ones were necessary and which ones were unnecessary and to ensure that they were suitable for the data we needed to test in real life."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XznO2JyLc_7z"
   },
   "source": [
    "- We could decrease the learning rate after a certain epoch. However, since the loss and validation accuracy levels did not fall to the desired level, doing so would not change the result much. This usage could be applied after reaching a desired point, so that fine calculations could be made and overfitting would not occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Id_OK-OndXhl"
   },
   "source": [
    "- We could have learned finer details by applying more filters, but when we tried this, we saw that the model was quickly overfitting, so this method was not successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRSuH_PhblsG"
   },
   "source": [
    "### 1.8 Kaggle Test Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L1Wj1K1gsRG3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the paths of test set images\n",
    "test_dir = 'test-images2' # Adjust the path to your test images directory\n",
    "test_image_paths = [os.path.join(test_dir, img_name) for img_name in os.listdir(test_dir)]\n",
    "# Sort the filenames numerically\n",
    "sorted_files = sorted(test_image_paths, key=lambda x: int(''.join(filter(str.isdigit, x))))\n",
    "\n",
    "# Step 2: Preprocess the test set images\n",
    "test_images = []\n",
    "for img_path in sorted_files:\n",
    "    img = Image.open(img_path).convert('RGB').resize((256, 256))  # Ensure image is in RGB format\n",
    "    img = transforms.ToTensor()(img)  # Convert to tensor\n",
    "    img = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(img)  # Normalize pixel values\n",
    "    test_images.append(img)\n",
    "\n",
    "\n",
    "# Convert the list of images to a single tensor\n",
    "test_images = torch.stack(test_images)\n",
    "\n",
    "# Step 3: Load the best performing model\n",
    "model = cnn_model\n",
    "model.eval()\n",
    "\n",
    "# Step 4: Predict class labels for test set images\n",
    "predictions = []\n",
    "for image in test_images:\n",
    "    image = image.to(device)\n",
    "    image = image.unsqueeze(0)  # Add batch dimension\n",
    "    output = model(image)\n",
    "    predicted_class = output.argmax(dim=1).item()  # Find the index with maximum score\n",
    "    predictions.append(predicted_class)\n",
    "\n",
    "# Step 5: Map predicted class labels to corresponding class names\n",
    "class_labels = {\n",
    "    0: 'cane', 1: 'cavallo', 2: 'elefante', 3: 'farfalla',\n",
    "    4: 'gallina', 5: 'gatto', 6: 'mucca', 7: 'pecora',\n",
    "    8: 'ragno', 9: 'scoiattolo'\n",
    "}\n",
    "\n",
    "# Step 6: Save predictions to CSV file\n",
    "df = pd.DataFrame({'ID': range(1, len(predictions) + 1), 'Label': [class_labels[p] for p in predictions]})\n",
    "df.to_csv('cnn_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Zc-z1RKeO7e"
   },
   "source": [
    "#### Kaggle Result: %57.7 (user: Kağan Soysal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jfa8FPbS8RCC"
   },
   "source": [
    "## 2. Exploring Transfer Learning with ResNet50 and MobileNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70saVqfas_xw"
   },
   "source": [
    "### 2.1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEfpAr_TeO7e"
   },
   "source": [
    "**TASK:** We will train certain parts of the ResNet18 and MobileNet models, which were previously trained using millions of images, on our own training set to make the models more suitable for our own dataset. In this way, the predictions will be more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qI6VGt0eO7f"
   },
   "source": [
    "##### Why do we freeze the rest and train only last layers?\n",
    "- The first layers learn general features (edge, corner) and are the same in every task. The last layers learn more specific details, so this is the part where we will adapt it to our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AUPSGkss_xx"
   },
   "source": [
    "### 2.2. Load the pre-trained ResNet18 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Rd21jcqJJIc",
    "outputId": "a8950600-5f8f-47e6-aad5-03e6b278e43c"
   },
   "outputs": [],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3nHpU7wSJJhG"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ujz4g2mJs_xx",
    "outputId": "224d46b9-7ed6-4d93-e15b-cdb8fe0d1b48",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Utilize torchvision library to load the pre-trained ResNet50 model\n",
    "## Ensure that the model's architecture matches ResNet50, by checking the model summary.\n",
    "res_net_18_model = models.resnet18(pretrained=True)\n",
    "summary(res_net_18_model, input_size=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6P7NdYbs_xx"
   },
   "source": [
    "### 2.3 Modify the ResNet18 model for transfer learning\n",
    "\n",
    "- As the first model we will only update the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F5G4ceUgP_Fs",
    "outputId": "ee4e27bd-bcaa-4c93-9253-acf1a2403115"
   },
   "outputs": [],
   "source": [
    "res_net_18_model_1 = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "## Freeze all layers of the ResNet18 model.\n",
    "for param in res_net_18_model_1.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "## Replace the final fully connected layer with a new FC layer matching the number of classes\n",
    "res_net_18_model_1.fc = nn.Linear(res_net_18_model_1.fc.in_features, 10)\n",
    "\n",
    "## Unfreeze the final FC layer\n",
    "for param in res_net_18_model_1.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "## Define appropriate loss function and optimizer for training\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(res_net_18_model_1.fc.parameters(), lr=0.001)\n",
    "\n",
    "## Train the modified ResNet18 model on the animal-10 image dataset. (base model)\n",
    "res_net_18_model_1 = res_net_18_model_1.to(device)\n",
    "\n",
    "train_losses_1 = []\n",
    "val_losses_1 = []\n",
    "val_accuracy_1 = []\n",
    "\n",
    "res_net_18_model_1.train()\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = res_net_18_model_1(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_losses_1.append(train_loss)\n",
    "\n",
    "        val_running_loss = 0.0\n",
    "\n",
    "\n",
    "        res_net_18_model_1.eval()\n",
    "\n",
    "        true_prediction = 0\n",
    "        total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "            val_outputs = res_net_18_model_1(val_inputs)\n",
    "            val_loss = loss_func(val_outputs, val_labels)\n",
    "            val_running_loss += val_loss.item()\n",
    "            _, predicted_target = torch.max(val_outputs.data, 1)\n",
    "            total += val_labels.size(0)\n",
    "            true_prediction += (predicted_target == val_labels).sum().item()\n",
    "\n",
    "    val_loss = val_running_loss / len(val_loader)\n",
    "    val_losses_1.append(val_loss)\n",
    "    val_accuracy = 100 * true_prediction / total\n",
    "    val_accuracy_1.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqFaXV5TeO7g"
   },
   "source": [
    "- Although we already had a high accuracy of 81% at the beginning, thanks to the process we carried out, we increased the validation accuracy to 92%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "fm_A_pNiygc3",
    "outputId": "ef515724-b898-448e-f180-72dc8d197799"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses_1, label='Training Loss')\n",
    "plt.plot(val_losses_1, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "mO1QcOleygp0",
    "outputId": "b01fa4a8-bf97-4f9c-e2b4-9cc798013c69"
   },
   "outputs": [],
   "source": [
    "plt.plot(val_accuracy_1, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeA3m3WOeO7g"
   },
   "source": [
    "- As the second model, we will update the 3rd and 4th convolutional layers as well as the last layer.\n",
    "\n",
    "- Since we will be updating more layers, we reduce the learning rate slightly compared to the first model, otherwise we may corrupt the already learned parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FlEHAyNX0QP0",
    "outputId": "e85ec6c8-fd69-424d-cbfc-de2a9e97cdd9"
   },
   "outputs": [],
   "source": [
    "## Define another ResNet18 model\n",
    "res_net_18_model_2 = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "## Freeze all layers of the ResNet18 model.\n",
    "for param in res_net_18_model_2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "## Replace the final fully connected layer with a new FC layer matching the number of classes\n",
    "res_net_18_model_2.fc = nn.Linear(res_net_18_model_2.fc.in_features, 10)\n",
    "\n",
    "## Unfreeze the final FC layer\n",
    "for param in res_net_18_model_2.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "## Unfreeze convolutional layers 3 and 4 of the ResNet18 model and again proceed with training. (second model)\n",
    "for param in res_net_18_model_2.layer3.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in res_net_18_model_2.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, res_net_18_model_2.parameters()), lr=0.0001)\n",
    "\n",
    "res_net_18_model_2 = res_net_18_model_2.to(device)\n",
    "\n",
    "train_losses_2 = []\n",
    "val_losses_2 = []\n",
    "val_accuracy_2 = []\n",
    "\n",
    "res_net_18_model_2.train()\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = res_net_18_model_2(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_losses_2.append(train_loss)\n",
    "\n",
    "    val_running_loss = 0.0\n",
    "\n",
    "    res_net_18_model_2.eval()\n",
    "\n",
    "    true_prediction = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "            val_outputs = res_net_18_model_2(val_inputs)\n",
    "            val_loss = loss_func(val_outputs, val_labels)\n",
    "            val_running_loss += val_loss.item()\n",
    "            _, predicted_target = torch.max(val_outputs.data, 1)\n",
    "            total += val_labels.size(0)\n",
    "            true_prediction += (predicted_target == val_labels).sum().item()\n",
    "\n",
    "    val_loss = val_running_loss / len(val_loader)\n",
    "    val_losses_2.append(val_loss)\n",
    "    val_accuracy = 100 * true_prediction / total\n",
    "    val_accuracy_2.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jeiyc8OveO7g"
   },
   "source": [
    "- Likewise, we have moved the accuracy value to an even higher level with the operation we have performed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "knj1HdKeymm2",
    "outputId": "87364c47-5ad9-4003-d3c8-a28e14acf72c"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses_2, label='Training Loss')\n",
    "plt.plot(val_losses_2, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "-zXbqOxUymuG",
    "outputId": "c2bd2d93-dfa8-42e7-996c-4185c69ccce1"
   },
   "outputs": [],
   "source": [
    "plt.plot(val_accuracy_2, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ADQzytyeO7g"
   },
   "source": [
    "- As the 3rd model, we will update all layers without freezing any layer.\n",
    "\n",
    "- We make the adjustment more subtle by making the learning rate a little smaller than in other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nCnETnfayW4A",
    "outputId": "ed9b254a-81b2-44e7-fd54-cd471344cdcb"
   },
   "outputs": [],
   "source": [
    "## Define another ResNet18 model\n",
    "res_net_18_model_3 = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "## Replace the final fully connected layer with a new FC layer matching the number of classes proceed with training. (third model)\n",
    "res_net_18_model_3.fc = nn.Linear(res_net_18_model_3.fc.in_features, 10)\n",
    "\n",
    "res_net_18_model_3 = res_net_18_model_3.to(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(res_net_18_model_3.parameters(), lr=0.00005)\n",
    "\n",
    "train_losses_3 = []\n",
    "val_losses_3 = []\n",
    "val_accuracy_3 = []\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    res_net_18_model_3.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = res_net_18_model_3(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_losses_3.append(train_loss)\n",
    "\n",
    "    val_running_loss = 0.0\n",
    "\n",
    "    res_net_18_model_3.eval()\n",
    "\n",
    "    true_prediction = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "            val_outputs = res_net_18_model_3(val_inputs)\n",
    "            val_loss = loss_func(val_outputs, val_labels)\n",
    "            val_running_loss += val_loss.item()\n",
    "            _, predicted_target = torch.max(val_outputs.data, 1)\n",
    "            total += val_labels.size(0)\n",
    "            true_prediction += (predicted_target == val_labels).sum().item()\n",
    "\n",
    "    val_loss = val_running_loss / len(val_loader)\n",
    "    val_losses_3.append(val_loss)\n",
    "    val_accuracy = 100 * true_prediction / total\n",
    "    val_accuracy_3.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "STcMU2hDxg0n",
    "outputId": "4bc03711-d31b-47c2-f228-62f9473c9bd7"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses_3, label='Training Loss')\n",
    "plt.plot(val_losses_3, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "fVBhzMtHxoI2",
    "outputId": "9eadc46a-2576-4c23-9e45-e6d7a14e9ec2"
   },
   "outputs": [],
   "source": [
    "plt.plot(val_accuracy_3, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zodp-puH3wzC"
   },
   "source": [
    "- Validation accuracy was slightly higher in the 3rd model compared to the others. Since we updated all layers, we made the entire model suitable for our own dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biAtQZdWs_xy"
   },
   "source": [
    "### 2.4 Evaluate the fine-tuned ResNet18 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4DYImwx7s_xy",
    "outputId": "fe5aaead-356b-4675-9cd9-f835fd59a9ac"
   },
   "outputs": [],
   "source": [
    "## Test the best model on the test set to evaluate its performance.\n",
    "res_net_18_model_3.eval()\n",
    "\n",
    "true_prediction = 0\n",
    "total = 0\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = res_net_18_model_3(inputs)\n",
    "        _, predicted_target = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        true_prediction += (predicted_target == labels).sum().item()\n",
    "\n",
    "        all_preds.extend(predicted_target.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(f\"Accuracy: {100 * true_prediction / total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "id": "2zWXrls_AC4g",
    "outputId": "cf7fae07-e737-4344-aa01-ef35f828d8d6"
   },
   "outputs": [],
   "source": [
    "## Compute metrics such as accuracy, precision, recall, and F1-score to assess classification performance.\n",
    "report_resnet18 = classification_report(all_labels, all_preds, target_names=['cane', 'cavallo', 'elefante', 'farfalla', 'gallina', 'gatto', 'mucca', 'pecora', 'ragno', 'scoiattolo'], output_dict = True)\n",
    "df_report_resnet18 = pd.DataFrame(report_resnet18).transpose()\n",
    "df_report_resnet18.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "tACXcolPC7Yw",
    "outputId": "266d3418-9092-43d0-a920-bdcd72bafd5c"
   },
   "outputs": [],
   "source": [
    "## Visualize confusion matrix to understand the model's behavior across different classes\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['cane', 'cavallo', 'elefante', 'farfalla', 'gallina', 'gatto', 'mucca', 'pecora', 'ragno', 'scoiattolo'], yticklabels=['cane', 'cavallo', 'elefante', 'farfalla', 'gallina', 'gatto', 'mucca', 'pecora', 'ragno', 'scoiattolo'])\n",
    "plt.ylabel('Real Labels')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOIVXExoeO7h"
   },
   "source": [
    "### 2.5 Kaggle Test Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YANgrUrN4lrC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the paths of test set images\n",
    "test_dir = 'test-images2' # Adjust the path to your test images directory\n",
    "test_image_paths = [os.path.join(test_dir, img_name) for img_name in os.listdir(test_dir)]\n",
    "# Sort the filenames numerically\n",
    "sorted_files = sorted(test_image_paths, key=lambda x: int(''.join(filter(str.isdigit, x))))\n",
    "\n",
    "# Step 2: Preprocess the test set images\n",
    "test_images = []\n",
    "for img_path in sorted_files:\n",
    "    img = Image.open(img_path).convert('RGB').resize((256, 256))  # Ensure image is in RGB format\n",
    "    img = transforms.ToTensor()(img)  # Convert to tensor\n",
    "    img = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(img)  # Normalize pixel values\n",
    "    test_images.append(img)\n",
    "\n",
    "\n",
    "# Convert the list of images to a single tensor\n",
    "test_images = torch.stack(test_images)\n",
    "\n",
    "# Step 3: Load the best performing model\n",
    "model = res_net_18_model_3\n",
    "model.eval()\n",
    "\n",
    "# Step 4: Predict class labels for test set images\n",
    "predictions = []\n",
    "for image in test_images:\n",
    "    image = image.to(device)\n",
    "    image = image.unsqueeze(0)  # Add batch dimension\n",
    "    output = model(image)\n",
    "    predicted_class = output.argmax(dim=1).item()  # Find the index with maximum score\n",
    "    predictions.append(predicted_class)\n",
    "\n",
    "# Step 5: Map predicted class labels to corresponding class names\n",
    "class_labels = {\n",
    "    0: 'cane', 1: 'cavallo', 2: 'elefante', 3: 'farfalla',\n",
    "    4: 'gallina', 5: 'gatto', 6: 'mucca', 7: 'pecora',\n",
    "    8: 'ragno', 9: 'scoiattolo'\n",
    "}\n",
    "\n",
    "# Step 6: Save predictions to CSV file\n",
    "df = pd.DataFrame({'ID': range(1, len(predictions) + 1), 'Label': [class_labels[p] for p in predictions]})\n",
    "df.to_csv('resnet18_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovMyrE5qeO7h"
   },
   "source": [
    "#### Kaggle Result: %93.2 (user: Kağan Soysal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "998AhkSqxRLr"
   },
   "source": [
    "### 2.7. Load the pre-trained MobileNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwyz8eZ_Erwb"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eas9qmLFxRLs",
    "outputId": "0882e255-912e-4af4-b32a-668c333dc402"
   },
   "outputs": [],
   "source": [
    "## Utilize torchvision library to load the pre-trained MobileNetV2 model\n",
    "## Ensure that the model's architecture matches MobileNetV2, by checking the model summary.\n",
    "mobile_net_1 = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "summary(mobile_net_1, input_size=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkIZHLEXxcFu"
   },
   "source": [
    "### 2.8 Modify the MobileNet model for transfer learning\n",
    "\n",
    "- In the first mobile net model we will only update the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tFW5CF46xcFv",
    "outputId": "138d7501-25ed-4a6c-bfb5-5c3ba59c8fda"
   },
   "outputs": [],
   "source": [
    "## Freeze all layers of the MobileNet model.\n",
    "for param in mobile_net_1.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "## Replace the final fully connected layer with a new FC layer matching the number of classes\n",
    "mobile_net_1.classifier[1] = nn.Linear(mobile_net_1.last_channel, 10)\n",
    "\n",
    "## Unfreeze the final FC layer\n",
    "for param in mobile_net_1.classifier[1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "## Define appropriate loss function and optimizer for training\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mobile_net_1.classifier[1].parameters(), lr=0.001)\n",
    "\n",
    "## Train the modified MobileNet model on the animal-10 image dataset. (base model)\n",
    "epochs = 30\n",
    "train_losses_1 = []\n",
    "val_losses_1 = []\n",
    "val_accuracy_1 = []\n",
    "\n",
    "mobile_net_1 = mobile_net_1.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    mobile_net_1.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mobile_net_1(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_losses_1.append(train_loss)\n",
    "\n",
    "    mobile_net_1.eval()\n",
    "    val_running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "            val_outputs = mobile_net_1(val_inputs)\n",
    "            val_loss = loss_func(val_outputs, val_labels)\n",
    "            val_running_loss += val_loss.item()\n",
    "\n",
    "            _, predicted_target = torch.max(val_outputs.data, 1)\n",
    "            total += val_labels.size(0)\n",
    "            correct += (predicted_target == val_labels).sum().item()\n",
    "\n",
    "    val_loss = val_running_loss / len(val_loader)\n",
    "    val_losses_1.append(val_loss)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    val_accuracy_1.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "SzndWbORJPNo",
    "outputId": "dd7b35d0-9213-4fd3-c1c9-977508bf35a8"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses_1, label='Training Loss')\n",
    "plt.plot(val_losses_1, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "cV3tAp-qJhIK",
    "outputId": "cc00c416-594d-422e-bc12-f05418fe231c"
   },
   "outputs": [],
   "source": [
    "plt.plot(val_accuracy_1, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sib4eEgReO7i"
   },
   "source": [
    "- In the second mobile net model we will update all layers of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Mf3sXu7ygD1",
    "outputId": "2b06b799-13d1-4754-cd97-48619e8adc80"
   },
   "outputs": [],
   "source": [
    "## Define another MobileNet model\n",
    "mobile_net_2 = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "\n",
    "## Replace the final fully connected layer with a new FC layer matching the number of classes proceed with training. (second model)\n",
    "mobile_net_2.classifier[1] = nn.Linear(mobile_net_2.last_channel, 10)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mobile_net_2.parameters(), lr=0.0001)\n",
    "\n",
    "epochs = 10\n",
    "train_losses_2 = []\n",
    "val_losses_2 = []\n",
    "val_accuracy_2 = []\n",
    "\n",
    "mobile_net_2 = mobile_net_2.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    mobile_net_2.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mobile_net_2(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_losses_2.append(train_loss)\n",
    "\n",
    "    mobile_net_2.eval()\n",
    "    val_running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "            val_outputs = mobile_net_2(val_inputs)\n",
    "            val_loss = loss_func(val_outputs, val_labels)\n",
    "            val_running_loss += val_loss.item()\n",
    "\n",
    "            _, predicted_target = torch.max(val_outputs.data, 1)\n",
    "            total += val_labels.size(0)\n",
    "            correct += (predicted_target == val_labels).sum().item()\n",
    "\n",
    "    val_loss = val_running_loss / len(val_loader)\n",
    "    val_losses_2.append(val_loss)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    val_accuracy_2.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "4J_0h1S_JYQ7",
    "outputId": "03e770a0-a1dc-4e84-f2cb-5b1a1a217a9b"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses_2, label='Training Loss')\n",
    "plt.plot(val_losses_2, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "J60tiCBeNpwH",
    "outputId": "c76a264f-7c57-4793-dfd6-930d0b9c3cb4"
   },
   "outputs": [],
   "source": [
    "plt.plot(val_accuracy_2, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBEZnnsXxcFy"
   },
   "source": [
    "- Since we updated all layers in the second model, it provided higher accuracy on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jnv5mIdtyr71"
   },
   "source": [
    "### 2.9 Evaluate the fine-tuned MobileNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FIs86Z-vyr72",
    "outputId": "7762353a-f492-4218-db91-97f574febea7"
   },
   "outputs": [],
   "source": [
    "## Test the best model on the test set to evaluate its performance.\n",
    "mobile_net_2.eval()\n",
    "\n",
    "true_prediction = 0\n",
    "total = 0\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = mobile_net_2(inputs)\n",
    "        _, predicted_target = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        true_prediction += (predicted_target == labels).sum().item()\n",
    "\n",
    "        all_preds.extend(predicted_target.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(f\"Accuracy: {100 * true_prediction / total}%\")\n",
    "\n",
    "## Comment on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "id": "OWiwWZsdIVTt",
    "outputId": "2caa830f-94ac-491e-de5a-5bce636ee909"
   },
   "outputs": [],
   "source": [
    "## Compute metrics such as accuracy, precision, recall, and F1-score to assess classification performance.\n",
    "report = classification_report(all_labels, all_preds, target_names=['cane', 'cavallo', 'elefante', 'farfalla', 'gallina', 'gatto', 'mucca', 'pecora', 'ragno', 'scoiattolo'], output_dict = True)\n",
    "df_report_mobilenet = pd.DataFrame(report).transpose()\n",
    "df_report_mobilenet.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "AB3P-16oIbkA",
    "outputId": "b9f01205-454e-4334-ac57-b40731d1f541"
   },
   "outputs": [],
   "source": [
    "## Visualize confusion matrix to understand the model's behavior across different classes\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['cane', 'cavallo', 'elefante', 'farfalla', 'gallina', 'gatto', 'mucca', 'pecora', 'ragno', 'scoiattolo'], yticklabels=['cane', 'cavallo', 'elefante', 'farfalla', 'gallina', 'gatto', 'mucca', 'pecora', 'ragno', 'scoiattolo'])\n",
    "plt.ylabel('Real Labels')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqxqlRDozR7q"
   },
   "source": [
    "### 2.10 Kaggle Test Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m-HO-G27P7OK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the paths of test set images\n",
    "test_dir = 'test-images2' # Adjust the path to your test images directory\n",
    "test_image_paths = [os.path.join(test_dir, img_name) for img_name in os.listdir(test_dir)]\n",
    "# Sort the filenames numerically\n",
    "sorted_files = sorted(test_image_paths, key=lambda x: int(''.join(filter(str.isdigit, x))))\n",
    "\n",
    "# Step 2: Preprocess the test set images\n",
    "test_images = []\n",
    "for img_path in sorted_files:\n",
    "    img = Image.open(img_path).convert('RGB').resize((256, 256))  # Ensure image is in RGB format\n",
    "    img = transforms.ToTensor()(img)  # Convert to tensor\n",
    "    img = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(img)  # Normalize pixel values\n",
    "    test_images.append(img)\n",
    "\n",
    "\n",
    "# Convert the list of images to a single tensor\n",
    "test_images = torch.stack(test_images)\n",
    "\n",
    "# Step 3: Load the best performing model\n",
    "model = mobile_net_2\n",
    "model.eval()\n",
    "\n",
    "# Step 4: Predict class labels for test set images\n",
    "predictions = []\n",
    "for image in test_images:\n",
    "    image = image.to(device)\n",
    "    image = image.unsqueeze(0)  # Add batch dimension\n",
    "    output = model(image)\n",
    "    predicted_class = output.argmax(dim=1).item()  # Find the index with maximum score\n",
    "    predictions.append(predicted_class)\n",
    "\n",
    "# Step 5: Map predicted class labels to corresponding class names\n",
    "class_labels = {\n",
    "    0: 'cane', 1: 'cavallo', 2: 'elefante', 3: 'farfalla',\n",
    "    4: 'gallina', 5: 'gatto', 6: 'mucca', 7: 'pecora',\n",
    "    8: 'ragno', 9: 'scoiattolo'\n",
    "}\n",
    "\n",
    "# Step 6: Save predictions to CSV file\n",
    "df = pd.DataFrame({'ID': range(1, len(predictions) + 1), 'Label': [class_labels[p] for p in predictions]})\n",
    "df.to_csv('mobilenet_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-BIKGSteO7j"
   },
   "source": [
    "#### Kaggle Result: %94.0 (user: Kağan Soysal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7N7lt6OG1cz7"
   },
   "source": [
    "## 3. Analyze advantages and disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQi6nJadeO7j"
   },
   "source": [
    "**Transfer Learning**\n",
    "\n",
    "*Advantages*:\n",
    "- Since it is pre-tuned, the training process takes a short time.\n",
    "\n",
    "- It gives good results even for small data sets because it has already learned the general features.\n",
    "\n",
    "*Disadvantages*:\n",
    "- It is inefficient if there are significant differences between the dataset it was previously trained on and the target dataset.\n",
    "\n",
    "- If the model is very large and complex, it can memorize the target dataset.\n",
    "\n",
    "**Training from Scratch**\n",
    "\n",
    "*Advantages*:\n",
    "- Since it is trained from scratch, it can fully adapt to the dataset\n",
    "\n",
    "- It can be specially prepared for special problems.\n",
    "\n",
    "*Disadvantages*:\n",
    "- Requires a lot of time and computational resources.\n",
    "\n",
    "- Requires a large and diverse dataset for high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "id": "vrTY0X3EzIef",
    "outputId": "54996da8-86d3-4adb-f85d-7e02643fa271"
   },
   "outputs": [],
   "source": [
    "## Compare the best fine-tuned MobileNet model performance with the best CNN model implemented from scratch\n",
    "result_df1 = pd.concat([df_report_mobilenet, df_report_cnn], axis=1, ignore_index=False)\n",
    "result_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "id": "VhHWTpxcILMY",
    "outputId": "f105ddf6-0856-4770-cd70-364e154ec1f9"
   },
   "outputs": [],
   "source": [
    "## Compare the best fine-tuned MobileNet model performance with the best ResNet18 model implemented from scratch\n",
    "result_df2 = pd.concat([df_report_mobilenet, df_report_resnet18], axis=1, ignore_index=False)\n",
    "result_df2"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
